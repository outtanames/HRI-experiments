{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f409957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for video files in: /Users/scottloftin/dev/HRI-experiments/video_files\n",
      "Placing audio files in: /Users/scottloftin/dev/HRI-experiments/audio_files\n",
      "Found session_46_clipped.mov, converting...\n",
      "Running conversion: ffmpeg -i /Users/scottloftin/dev/HRI-experiments/video_files/session_46_clipped.mov -ab 160k -ac 2 -ar 16000 -vn /Users/scottloftin/dev/HRI-experiments/audio_files/session_46_clipped.wav\n",
      "Found robot_talking.mp4, converting...\n",
      "Running conversion: ffmpeg -i /Users/scottloftin/dev/HRI-experiments/video_files/robot_talking.mp4 -ab 160k -ac 2 -ar 16000 -vn /Users/scottloftin/dev/HRI-experiments/audio_files/robot_talking.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with clang version 4.0.1 (tags/RELEASE_401/final)\n",
      "  configuration: --prefix=/opt/concourse/worker/volumes/live/d5b9ea1c-8223-4ff6-7416-83e6b4cd6874/volume/ffmpeg_1587154914508/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol --cc=x86_64-apple-darwin13.4.0-clang --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/Users/scottloftin/dev/HRI-experiments/video_files/session_46_clipped.mov':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    creation_time   : 2023-02-01T03:53:49.000000Z\n",
      "  Duration: 00:00:22.92, start: 0.000000, bitrate: 4538 kb/s\n",
      "    Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, mono, fltp, 65 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-02-01T03:53:49.000000Z\n",
      "      handler_name    : Core Media Audio\n",
      "    Stream #0:1(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, smpte170m/smpte170m/bt709), 640x360, 4463 kb/s, 30.05 fps, 30 tbr, 1k tbn, 2k tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-02-01T03:53:49.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "      encoder         : H.264\n",
      "File '/Users/scottloftin/dev/HRI-experiments/audio_files/session_46_clipped.wav' already exists. Overwrite ? [y/N] Not overwriting - exiting\n",
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with clang version 4.0.1 (tags/RELEASE_401/final)\n",
      "  configuration: --prefix=/opt/concourse/worker/volumes/live/d5b9ea1c-8223-4ff6-7416-83e6b4cd6874/volume/ffmpeg_1587154914508/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol --cc=x86_64-apple-darwin13.4.0-clang --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/Users/scottloftin/dev/HRI-experiments/video_files/robot_talking.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    creation_time   : 2023-02-05T23:45:21.000000Z\n",
      "  Duration: 00:00:32.89, start: 0.000000, bitrate: 15944 kb/s\n",
      "    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080, 15764 kb/s, 30.02 fps, 30 tbr, 1k tbn, 2k tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-02-05T23:45:21.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, mono, fltp, 69 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-02-05T23:45:21.000000Z\n",
      "      handler_name    : Core Media Audio\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/Users/scottloftin/dev/HRI-experiments/audio_files/robot_talking.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, stereo, s16, 512 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-02-05T23:45:21.000000Z\n",
      "      handler_name    : Core Media Audio\n",
      "      encoder         : Lavc58.54.100 pcm_s16le\n",
      "size=    2055kB time=00:00:32.88 bitrate= 512.0kbits/s speed=1.23e+03x    \n",
      "video:0kB audio:2055kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.003706%\n"
     ]
    }
   ],
   "source": [
    "# start by extracting the raw audio with ffmpeg\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_dir = os.path.join(Path.home(), Path(\"dev/HRI-experiments\"))\n",
    "video_dir = os.path.join(project_dir, \"video_files\")\n",
    "print(\"Searching for video files in: \" + video_dir)\n",
    "audio_dir = os.path.join(project_dir, \"audio_files\")\n",
    "print(\"Placing audio files in: \" + audio_dir)\n",
    "\n",
    "for in_file in os.listdir(video_dir):\n",
    "    print(\"Found \" + in_file + \", converting...\")\n",
    "    in_file = os.path.join(video_dir, in_file)\n",
    "    out_file = os.path.join(audio_dir, Path(in_file).stem + \".wav\")\n",
    "    convert_command = \"ffmpeg -i \" + in_file + \" -ab 160k -ac 2 -ar 16000 -vn \" + out_file\n",
    "    print(\"Running conversion: \" + convert_command)\n",
    "    subprocess.call(convert_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8630bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(152576,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets load a wav file as numpy array\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "audio_filename = os.path.join(audio_dir, \"Counting_test.wav\")\n",
    "output = wavfile.read(audio_filename)\n",
    "sampling_rate = output[0]\n",
    "print(sampling_rate)\n",
    "# This needs to be set for either stereo or mono\n",
    "# samples = [a[1] for a in output[1]]\n",
    "audio_data = np.array(output[1], dtype=float)\n",
    "audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12755994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "704e44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "inputs = processor(audio_data, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c31615ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one two three four five six seven h']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96f04356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.speech_to_text_2.modeling_speech_to_text_2.Speech2Text2ForCausalLM'> is overwritten by shared decoder config: Speech2Text2Config {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"Speech2TextForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"conv_channels\": 1024,\n",
      "  \"conv_kernel_sizes\": [\n",
      "    5,\n",
      "    5\n",
      "  ],\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 7,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"input_channels\": 1,\n",
      "  \"input_feat_per_channel\": 80,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_source_positions\": 6000,\n",
      "  \"max_target_positions\": 1024,\n",
      "  \"model_type\": \"speech_to_text_2\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_conv_layers\": 2,\n",
      "  \"num_hidden_layers\": 7,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n",
    "processor = Speech2Text2Processor.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n",
    "\n",
    "\n",
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch\n",
    "\n",
    "\n",
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# ds = ds.map(map_to_array)\n",
    "\n",
    "# inputs = processor(ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"pt\")\n",
    "inputs = processor(audio_data, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaa15cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s> Ich habe mir die Frage gestellt, dass es in diesem Raum zu sehen ist, die man in der Lage ist, in der Nähe zu lernen, die man mit einem neuen Gespräch zu lernen. </s>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da908876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
